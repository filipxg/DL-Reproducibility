{"cells":[{"cell_type":"code","execution_count":114,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T09:36:05.531755Z","iopub.status.busy":"2024-04-11T09:36:05.531373Z","iopub.status.idle":"2024-04-11T09:36:05.536949Z","shell.execute_reply":"2024-04-11T09:36:05.535718Z","shell.execute_reply.started":"2024-04-11T09:36:05.531725Z"},"trusted":true},"outputs":[],"source":["import os\n","import sys\n","sys.path.insert(1, \"/kaggle/input/3d-unet\")"]},{"cell_type":"code","execution_count":115,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-11T09:36:09.034302Z","iopub.status.busy":"2024-04-11T09:36:09.033570Z","iopub.status.idle":"2024-04-11T09:36:09.038982Z","shell.execute_reply":"2024-04-11T09:36:09.037817Z","shell.execute_reply.started":"2024-04-11T09:36:09.034270Z"},"trusted":true},"outputs":[],"source":["import torch\n","import cv2\n","import numpy as np\n","from skimage import io\n","from pytorch3dunet.unet3d.model import UNet3D"]},{"cell_type":"markdown","metadata":{},"source":["<h1> Dataset and auxiliary functions </h1>"]},{"cell_type":"code","execution_count":117,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T09:36:36.898476Z","iopub.status.busy":"2024-04-11T09:36:36.897696Z","iopub.status.idle":"2024-04-11T09:36:36.904363Z","shell.execute_reply":"2024-04-11T09:36:36.903351Z","shell.execute_reply.started":"2024-04-11T09:36:36.898443Z"},"trusted":true},"outputs":[],"source":["def binarize(mask: torch.tensor) -> torch.tensor:\n","    \"\"\"\n","    Converts instance segmentation labels, i.e. 1,2,3,4...\n","    into semantic segmentation labels, i.e. 0,1\n","    because instance segmentation is supposed to be done in post-processing,\n","    as described in the QCANet paper.\n","    \"\"\"\n","    return torch.where(mask != 0, 1, 0)"]},{"cell_type":"code","execution_count":118,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T09:36:39.554197Z","iopub.status.busy":"2024-04-11T09:36:39.553814Z","iopub.status.idle":"2024-04-11T09:36:39.571225Z","shell.execute_reply":"2024-04-11T09:36:39.570140Z","shell.execute_reply.started":"2024-04-11T09:36:39.554166Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from scipy.ndimage import zoom\n","import cv2\n","import torch.nn.functional as F\n","\n","class EmbryoDataset(Dataset):\n","    \n","    def __init__(self, image_dir_path: str, gt_dir_path: str, augment: bool = False):\n","        self.image_dir_path = image_dir_path\n","        self.gt_dir_path = gt_dir_path\n","        self._filenames = os.listdir(image_dir_path)\n","        self._augment = augment\n","        \n","        # If augmentation is used, the filenames are duplicated because rotated versions of\n","        # each image are produced, as described in the QCANet paper\n","        if self._augment:\n","            self._filenames = self._filenames * 4\n","        \n","    def __len__(self):\n","        return len(self._filenames)\n","    \n","    def __preprocess_image(self, image: torch.Tensor) -> torch.Tensor:\n","        # Preprocessing images as described in the QCANet paper\n","        min_value = torch.min(image)\n","        max_value = torch.max(image)\n","        \n","        image = (image - min_value) / (max_value - min_value)\n","        \n","        return image\n","    \n","    def __preprocess_gt(self, gt: torch.Tensor) -> torch.Tensor:\n","        # Ground truth preprocessing\n","        gt = binarize(gt)\n","        \n","        return gt\n","    \n","    def __preprocess(self, raw: np.ndarray, type: int) -> np.ndarray:\n","        preprocessed = raw.astype(np.float32)\n","        x, y = preprocessed.shape[1], preprocessed.shape[2]\n","        x_scale_factor = 128 / x\n","        y_scale_factor = 128 / y\n","        # Z-axis stretched by a factor of 2.1875 as described in the QCANet paper\n","        # Y and X axes normalized to 128 pixels to keep sizes consistent between samples\n","        preprocessed = zoom(preprocessed, (2.1875, x_scale_factor, y_scale_factor), order=3)\n","        \n","        preprocessed = torch.from_numpy(preprocessed)\n","        preprocessed = preprocessed.to(\"cuda\", dtype=torch.float32)\n","        preprocessed = preprocessed.unsqueeze(dim=0)\n","        \n","        if type == 0:\n","            preprocessed = self.__preprocess_image(preprocessed)\n","        if type == 1:\n","            preprocessed = self.__preprocess_gt(preprocessed)\n","            \n","        return preprocessed\n","\n","    def __getitem__(self, idx):\n","        # With augmentation enabled, we take 4 copies of one image with different\n","        # rotation for each one\n","        if self._augment:\n","            original_idx = idx // 4\n","        else:\n","            original_idx = idx\n","\n","        curr_filename = self._filenames[original_idx]\n","        curr_image_path = os.path.join(self.image_dir_path, curr_filename)\n","        curr_gt_path = os.path.join(self.gt_dir_path, curr_filename)\n","\n","        image_raw = io.imread(curr_image_path)\n","        gt_raw = io.imread(curr_gt_path)\n","        \n","        x = self.__preprocess(image_raw, type=0)\n","        y = self.__preprocess(gt_raw, type=1)\n","\n","        # With augmentation enabled, we take 4 copies of one image with different\n","        # rotation for each one\n","        if self._augment:\n","            transform_type = idx % 4\n","        \n","            # Apply the specific transformation\n","            if transform_type == 1:  # Flip horizontally\n","                x = transforms.functional.hflip(x)\n","                y = transforms.functional.hflip(y)\n","            elif transform_type == 2:  # Flip vertically\n","                x = transforms.functional.vflip(x)\n","                y = transforms.functional.vflip(y)\n","            elif transform_type == 3:  # Flip both horizontally and vertically\n","                x = transforms.functional.hflip(x)\n","                y = transforms.functional.hflip(y)\n","        \n","        return (x.cuda(), y.cuda())"]},{"cell_type":"code","execution_count":119,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T09:36:41.254252Z","iopub.status.busy":"2024-04-11T09:36:41.253531Z","iopub.status.idle":"2024-04-11T09:36:41.263146Z","shell.execute_reply":"2024-04-11T09:36:41.262160Z","shell.execute_reply.started":"2024-04-11T09:36:41.254216Z"},"trusted":true},"outputs":[],"source":["train_set = EmbryoDataset(image_dir_path=\"/kaggle/input/dl-reprod/Images/train/Images\", gt_dir_path=\"/kaggle/input/dl-reprod/GroundTruth/train/GroundTruth_NSN\", augment=True)\n","test_set = EmbryoDataset(image_dir_path=\"/kaggle/input/dl-reprod/Images/test/Images\", gt_dir_path=\"/kaggle/input/dl-reprod/GroundTruth/test/GroundTruth_QCANet\", augment=False)\n","train_loader = DataLoader(train_set, batch_size=1, shuffle=True)\n","test_loader = DataLoader(test_set, batch_size=1, shuffle=False)"]},{"cell_type":"markdown","metadata":{},"source":["<h1> Metric functions </h1>"]},{"cell_type":"markdown","metadata":{},"source":["DiceLoss was used as a loss function in the QCANet paper. Implemented based on https://arxiv.org/pdf/1606.04797.pdf\n","\n","$$D = \\frac{2\\sum_i^N p_i g_i}{\\sum_i^N p_i^2 + \\sum_i^N g_i^2}$$\n","\n","where the sums run over the $N$ voxels, of the predicted binary segmentation volume $p_i \\in P$ and the ground truth binary volume $g_i \\in G$."]},{"cell_type":"code","execution_count":120,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T09:36:43.941958Z","iopub.status.busy":"2024-04-11T09:36:43.941565Z","iopub.status.idle":"2024-04-11T09:36:43.948146Z","shell.execute_reply":"2024-04-11T09:36:43.947171Z","shell.execute_reply.started":"2024-04-11T09:36:43.941926Z"},"trusted":true},"outputs":[],"source":["def dice_loss(input, target):\n","    # Flatten the tensors to make sure you can sum over all voxels\n","    input_flat = input.view(-1)\n","    target_flat = target.view(-1)\n","    \n","    intersection = 2.0 * (input_flat * target_flat).sum()\n","    denominator = input_flat.pow(2).sum() + target_flat.pow(2).sum()\n","    \n","    dice_score = intersection / denominator.clamp(min=1e-6)\n","    return 1 - dice_score"]},{"cell_type":"code","execution_count":121,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T09:36:44.882421Z","iopub.status.busy":"2024-04-11T09:36:44.881511Z","iopub.status.idle":"2024-04-11T09:36:44.889572Z","shell.execute_reply":"2024-04-11T09:36:44.888643Z","shell.execute_reply.started":"2024-04-11T09:36:44.882374Z"},"trusted":true},"outputs":[],"source":["def iou(y_pred: torch.Tensor, y_gt: torch.Tensor, smooth=1e-6):\n","    \"\"\"\n","    Calculate Intersection over Union (IoU) for 3D semantic segmentation masks.\n","\n","    Parameters:\n","    - outputs: a tensor of shape (N, C, D, H, W) where\n","      N is the batch size,\n","      C is the number of classes,\n","      D is the depth,\n","      H and W are the height and width of the masks.\n","      The tensor should contain binary predictions (0 or 1).\n","    - labels: a tensor of the same shape as outputs containing the ground truth masks.\n","    - smooth: a small constant added to avoid division by zero.\n","\n","    Returns:\n","    - IoU: The Intersection over Union score for each class.\n","    \"\"\"\n","    # Ensure that both outputs and labels are booleans\n","    y_pred = y_pred > 0.5\n","    y_gt = y_gt > 0.5\n","    \n","    # Intersection and Union\n","    intersection = (y_pred & y_gt).float().sum(dim=(2, 3, 4)) # Sum over the spatial dimensions\n","    union = (y_pred | y_gt).float().sum(dim=(2, 3, 4)) # Sum over the spatial dimensions\n","    \n","    # Compute the IoU and handle cases where the union is 0\n","    iou = (intersection + smooth) / (union + smooth)\n","    \n","    return torch.mean(iou)  # Return the average IoU over the batch"]},{"cell_type":"markdown","metadata":{},"source":["<p> Bayesian hyperparameter optimization to determine hyperparameters not given in the paper </p>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T14:46:01.075696Z","iopub.status.busy":"2024-04-09T14:46:01.075401Z","iopub.status.idle":"2024-04-09T14:46:01.221716Z","shell.execute_reply":"2024-04-09T14:46:01.220366Z","shell.execute_reply.started":"2024-04-09T14:46:01.075670Z"},"trusted":true},"outputs":[],"source":["import optuna\n","from optuna.samplers import TPESampler\n","from tqdm import tqdm\n","\n","def objective(trial):\n","    # Suggested hyperparameters\n","    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n","    \n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Training loop\n","    model.train()\n","    for epoch in range(10):\n","        total_loss = 0\n","        for data in tqdm(train_loader):\n","            x, y = data\n","            optimizer.zero_grad()\n","            y_pred = model.forward(x)\n","            loss = dice_loss(y_pred, y)\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item()\n","        average_loss = total_loss / len(train_loader)\n","\n","    # Here you can also implement validation and use the validation loss as the return value\n","    return average_loss\n","\n","# Optuna study\n","study = optuna.create_study(direction=\"minimize\", sampler=TPESampler())\n","study.optimize(objective, n_trials=10)\n","\n","# Best trial result\n","print(\"Best trial:\")\n","trial = study.best_trial\n","print(f\"Value: {trial.value}\")\n","print(\"Params: \")\n","for key, value in trial.params.items():\n","    print(f\"    {key}: {value}\")\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["<h1> Components of the training loop </h1>"]},{"cell_type":"code","execution_count":122,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T09:36:46.571646Z","iopub.status.busy":"2024-04-11T09:36:46.571278Z","iopub.status.idle":"2024-04-11T09:36:46.584322Z","shell.execute_reply":"2024-04-11T09:36:46.583550Z","shell.execute_reply.started":"2024-04-11T09:36:46.571615Z"},"trusted":true},"outputs":[],"source":["model = UNet3D(in_channels=1, out_channels=1)\n","model.cuda()\n","# Learning rate determined by hyperparameter optimization below\n","optimizer = torch.optim.Adam(model.parameters(), lr=5e-05)"]},{"cell_type":"code","execution_count":123,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T09:36:48.262965Z","iopub.status.busy":"2024-04-11T09:36:48.261979Z","iopub.status.idle":"2024-04-11T09:36:48.269879Z","shell.execute_reply":"2024-04-11T09:36:48.268720Z","shell.execute_reply.started":"2024-04-11T09:36:48.262932Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","\n","def train_epoch():\n","    total_loss = 0\n","    with tqdm(total=len(train_set)) as pbar:\n","        for i, data in enumerate(train_loader):\n","            x, y = data\n","            optimizer.zero_grad()\n","            y_pred = model.forward(x)\n","            # loss, y_pred = model.forward(x, t=y, seg=False)\n","            loss = dice_loss(y_pred, y)\n","            loss.backward()\n","            optimizer.step()\n","            \n","            total_loss += loss.item()\n","            pbar.update(1)\n","    return total_loss / len(train_loader)"]},{"cell_type":"code","execution_count":124,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T09:36:48.984254Z","iopub.status.busy":"2024-04-11T09:36:48.983896Z","iopub.status.idle":"2024-04-11T09:36:48.990422Z","shell.execute_reply":"2024-04-11T09:36:48.989517Z","shell.execute_reply.started":"2024-04-11T09:36:48.984221Z"},"trusted":true},"outputs":[],"source":["def test_epoch():\n","    total_loss = 0\n","    total_iou = 0\n","    with torch.no_grad():\n","        for i, data in enumerate(test_loader):\n","            x_test, y_test = data\n","            y_pred = model(x_test)\n","            loss = dice_loss(y_pred, y_test)\n","            iou_value = iou(y_pred, y_test)\n","            total_loss += loss.item()\n","            total_iou += iou_value.item()\n","    return total_loss / len(test_loader), total_iou / len(test_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T09:36:53.513135Z","iopub.status.busy":"2024-04-11T09:36:53.512409Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 484/484 [13:50<00:00,  1.72s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/10, Train loss: 0.8472217211792291\n","Epoch 1/10, Test loss: 0.9174671877514232, Test IoU: 0.027402645331511103\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 484/484 [13:31<00:00,  1.68s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/10, Train loss: 0.819889424507283\n","Epoch 2/10, Test loss: 0.9122002856297926, Test IoU: 0.03608741985358806\n"]},{"name":"stderr","output_type":"stream","text":[" 18%|█▊        | 89/484 [02:29<10:57,  1.67s/it]"]}],"source":["losses = []\n","maxloss = 1\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","    model.train()\n","    train_loss = train_epoch()\n","    print(f'Epoch {epoch+1}/{num_epochs}, Train loss: {train_loss}')\n","    model.eval()\n","    test_loss, test_iou = test_epoch()\n","    print(f'Epoch {epoch+1}/{num_epochs}, Test loss: {test_loss}, Test IoU: {test_iou}')\n","    if (test_loss < maxloss):\n","        maxloss = test_loss\n","        torch.save(model.state_dict(), 'nsn_weights.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-28T20:57:16.938747Z","iopub.status.busy":"2024-03-28T20:57:16.938385Z","iopub.status.idle":"2024-03-28T20:57:17.143382Z","shell.execute_reply":"2024-03-28T20:57:17.142389Z","shell.execute_reply.started":"2024-03-28T20:57:16.938717Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.plot(losses)\n","plt.title('Training Loss Over Epochs')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.show()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4635605,"sourceId":7894709,"sourceType":"datasetVersion"},{"datasetId":4666593,"sourceId":7937918,"sourceType":"datasetVersion"},{"datasetId":4715431,"sourceId":8006387,"sourceType":"datasetVersion"},{"datasetId":4763184,"sourceId":8072219,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelInstanceId":19907,"sourceId":23800,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30664,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
